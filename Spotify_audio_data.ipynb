{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e097af1",
   "metadata": {},
   "source": [
    "### Audio data\n",
    "\n",
    "We have scraped the 30 seconds preview of the targetted songs from Deezer, and now we want to create a dataset with the embedding representation of all of these audios. We will leverage the MERT model and its encoder for the vector represenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51784349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef915afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the manifest dataset\n",
    "audio_manifest = pd.read_csv('audio_data/audio_manifest_20k.csv')\n",
    "\n",
    "# Drop the rows with missing values\n",
    "audio_manifest = audio_manifest.dropna()\n",
    "\n",
    "# change the audio path to the correct path\n",
    "audio_manifest['audio_path'] = 'audio_' + audio_manifest['audio_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93f9f0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MERT model\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "# Load the processor and model\n",
    "processor = AutoProcessor.from_pretrained(\"m-a-p/MERT-v1-330M\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"m-a-p/MERT-v1-330M\", trust_remote_code=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0107b969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting audio embedding extraction for 20405 songs...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding songs: 100%|██████████| 20405/20405 [1:15:38<00:00,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Audio embedding extraction completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract embeddings for all songs\n",
    "import torch\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "embeddings_list = []\n",
    "track_ids = []\n",
    "\n",
    "total_songs = len(audio_manifest)\n",
    "print(f\"Starting audio embedding extraction for {total_songs} songs...\\n\")\n",
    "\n",
    "for idx, row in tqdm(audio_manifest.iterrows(), total=total_songs, desc=\"Embedding songs\"):\n",
    "    track_id = row['track_id']\n",
    "    audio_path = row['audio_path']\n",
    "\n",
    "    # Log every 500 songs (adjust if you want)\n",
    "    #if idx % 500 == 0:\n",
    "    #    print(f\"[LOG] Processing song {idx}/{total_songs}: track_id={track_id}\")\n",
    "\n",
    "    # Load audio\n",
    "    waveform, sr = torchaudio.load(audio_path)\n",
    "\n",
    "    # Convert stereo → mono\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "    # Resample to 24kHz\n",
    "    if sr != 24000:\n",
    "        waveform = torchaudio.functional.resample(waveform, sr, 24000)\n",
    "\n",
    "    # Squeeze: [1, N] → [N]\n",
    "    waveform = waveform.squeeze(0)\n",
    "\n",
    "    # Prepare inputs\n",
    "    inputs = processor(waveform, sampling_rate=24000, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Embedding: mean over time → (1024,)\n",
    "    song_emb = outputs.last_hidden_state.mean(dim=1).squeeze(0).cpu().numpy()\n",
    "\n",
    "    # Store results\n",
    "    track_ids.append(track_id)\n",
    "    embeddings_list.append(song_emb)\n",
    "\n",
    "print(\"\\nAudio embedding extraction completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a27b74f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to audio_embeddings_mert_330M.parquet\n"
     ]
    }
   ],
   "source": [
    "# Create the embeddings dataframe\n",
    "audio_emb_df = pd.DataFrame(\n",
    "    np.vstack(embeddings_list),\n",
    "    columns=[f\"audio_emb_{i}\" for i in range(len(embeddings_list[0]))]\n",
    ")\n",
    "\n",
    "audio_emb_df.insert(0, 'track_id', track_ids)\n",
    "\n",
    "audio_emb_df.to_parquet(\"audio_data/audio_embeddings_mert_330M.parquet\", index=False)\n",
    "\n",
    "print(\"Embeddings saved to audio_embeddings_mert_330M.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dd5f5c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CHECK] Expected shape: (20405, 1025)\n",
      "[CHECK] Actual shape:   (20405, 1025)\n",
      "[PASS] audio_emb_df shape is correct!\n"
     ]
    }
   ],
   "source": [
    "# Sanity check of embedding df shape\n",
    "expected_rows = len(audio_manifest)\n",
    "expected_cols = 1 + len(embeddings_list[0])   # 1 = track_id\n",
    "\n",
    "actual_rows, actual_cols = audio_emb_df.shape\n",
    "\n",
    "print(f\"\\n[CHECK] Expected shape: ({expected_rows}, {expected_cols})\")\n",
    "print(f\"[CHECK] Actual shape:   ({actual_rows}, {actual_cols})\")\n",
    "\n",
    "if actual_rows == expected_rows and actual_cols == expected_cols:\n",
    "    print(\"[PASS] audio_emb_df shape is correct!\")\n",
    "else:\n",
    "    print(\"[FAIL] Shape mismatch!\")\n",
    "    if actual_rows != expected_rows:\n",
    "        print(f\"  → Row mismatch: expected {expected_rows}, got {actual_rows}\")\n",
    "    if actual_cols != expected_cols:\n",
    "        print(f\"  → Col mismatch: expected {expected_cols}, got {actual_cols}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
